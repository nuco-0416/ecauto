"""
SellerSprite ã‚«ãƒ†ã‚´ãƒªåˆ¥ASINè‡ªå‹•æŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2ï¼ˆå±¥æ­´ç®¡ç†æ©Ÿèƒ½ä»˜ãï¼‰

v1ã‹ã‚‰ã®æ”¹å–„ç‚¹:
- ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ãƒšãƒ¼ã‚¸å–å¾—å±¥æ­´ã‚’ç®¡ç†
- å‰å›ã®ç¶šãã‹ã‚‰æŠ½å‡ºã‚’å†é–‹å¯èƒ½
- ãƒšãƒ¼ã‚¸æ·±ã•ã‚’20ãƒšãƒ¼ã‚¸ã¾ã§æ‹¡å¼µå¯èƒ½ï¼ˆv1ã¯10ãƒšãƒ¼ã‚¸ã¾ã§ï¼‰
- å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰ã§é€²æ—ã‚’æ°¸ç¶šåŒ–

ä½¿ç”¨ä¾‹:
    # åˆå›å®Ÿè¡Œï¼ˆå±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼‰
    python sourcing/scripts/auto_extract_by_categories_v2.py \
      --target-new-asins 3000 \
      --pages-per-category 10 \
      --history-file category_history.json

    # å‰å›ã®ç¶šãã‹ã‚‰å®Ÿè¡Œï¼ˆ11ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰20ãƒšãƒ¼ã‚¸ç›®ã¾ã§ï¼‰
    python sourcing/scripts/auto_extract_by_categories_v2.py \
      --target-new-asins 3000 \
      --resume \
      --history-file category_history.json \
      --pages-per-category 20

å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼:
    {
      "categories": {
        "ã‚«ãƒ†ã‚´ãƒªå": {
          "nodeIdPaths": "[\"...\"]",
          "pages_extracted": 10,
          "last_updated": "2025-11-28T01:33:59",
          "asins_count": 458
        }
      },
      "metadata": {
        "total_asins": 3375,
        "last_run": "2025-11-28T01:54:58"
      }
    }
"""

import argparse
import asyncio
import sys
import sqlite3
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set, Optional
from collections import Counter
from dotenv import load_dotenv

# ecautoãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
env_path = project_root / 'sourcing' / 'sources' / 'sellersprite' / '.env'
load_dotenv(dotenv_path=env_path)

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sourcing.sources.sellersprite.utils.category_extractor import (
    build_product_research_url,
    create_browser_session
)


class CategoryHistoryManager:
    """ã‚«ãƒ†ã‚´ãƒªå±¥æ­´ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, history_file: Path):
        self.history_file = history_file
        self.history = self._load_history()

    def _load_history(self) -> Dict:
        """å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        if self.history_file.exists():
            with open(self.history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {
                "categories": {},
                "metadata": {
                    "total_asins": 0,
                    "last_run": None
                }
            }

    def save_history(self):
        """å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        self.history_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, ensure_ascii=False, indent=2)

    def get_category_info(self, category_name: str) -> Optional[Dict]:
        """ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—"""
        return self.history['categories'].get(category_name)

    def update_category(
        self,
        category_name: str,
        node_id_paths: str,
        pages_extracted: int,
        asins_count: int
    ):
        """ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’æ›´æ–°"""
        if category_name not in self.history['categories']:
            self.history['categories'][category_name] = {}

        self.history['categories'][category_name].update({
            'nodeIdPaths': node_id_paths,
            'pages_extracted': pages_extracted,
            'last_updated': datetime.now().isoformat(),
            'asins_count': asins_count
        })

    def update_metadata(self, total_asins: int):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        self.history['metadata']['total_asins'] = total_asins
        self.history['metadata']['last_run'] = datetime.now().isoformat()

    def get_categories_to_process(self, max_pages: int) -> List[tuple]:
        """
        å‡¦ç†ã™ã¹ãã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆã‚’å–å¾—

        Args:
            max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°

        Returns:
            [(category_name, category_info, start_page), ...]
        """
        categories_to_process = []

        for cat_name, cat_info in self.history['categories'].items():
            pages_extracted = cat_info.get('pages_extracted', 0)

            # ã¾ã  max_pages ã¾ã§åˆ°é”ã—ã¦ã„ãªã„ã‚«ãƒ†ã‚´ãƒªã‚’å¯¾è±¡
            if pages_extracted < max_pages:
                start_page = pages_extracted + 1
                categories_to_process.append((cat_name, cat_info, start_page))

        return categories_to_process


async def extract_asins_with_categories_paginated(
    page,
    limit: int,
    start_page: int = 1
) -> List[Dict[str, str]]:
    """
    ãƒšãƒ¼ã‚¸ãƒ³ã‚°å¯¾å¿œã®ASINæŠ½å‡ºï¼ˆé–‹å§‹ãƒšãƒ¼ã‚¸æŒ‡å®šå¯èƒ½ï¼‰

    Args:
        page: Playwrightãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        limit: å–å¾—ä»¶æ•°
        start_page: é–‹å§‹ãƒšãƒ¼ã‚¸ï¼ˆ1-20ï¼‰

    Returns:
        [{"asin": "B00XXXXX", "category": "...", "nodeIdPaths": "..."}, ...]
    """
    from sourcing.sources.sellersprite.utils.category_extractor import log

    all_data = []

    try:
        # ãƒªã‚¹ãƒˆè¡¨ç¤ºã«åˆ‡ã‚Šæ›¿ãˆ
        log("è¡¨ç¤ºã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã€Œãƒªã‚¹ãƒˆã€ã«åˆ‡ã‚Šæ›¿ãˆä¸­...")
        try:
            list_button = page.locator('div.el-button-group button').filter(has_text="ãƒªã‚¹ãƒˆ").first
            button_count = await list_button.count()

            if button_count > 0:
                button_text = await list_button.text_content()
                if button_text and button_text.strip() == "ãƒªã‚¹ãƒˆ":
                    await list_button.click()
                    await page.wait_for_timeout(2000)
                    log("[OK] ãƒªã‚¹ãƒˆè¡¨ç¤ºã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
        except Exception as e:
            log(f"[WARN] ãƒªã‚¹ãƒˆè¡¨ç¤ºã¸ã®åˆ‡ã‚Šæ›¿ãˆã‚¨ãƒ©ãƒ¼: {e}")

        # å¿…è¦ãªãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—
        pages_needed = (limit + 99) // 100
        end_page = min(start_page + pages_needed - 1, 20)  # æœ€å¤§20ãƒšãƒ¼ã‚¸ã¾ã§

        log(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³: {start_page}ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰{end_page}ãƒšãƒ¼ã‚¸ç›®ã¾ã§æŠ½å‡ºäºˆå®š")

        # é–‹å§‹ãƒšãƒ¼ã‚¸ã¾ã§ç§»å‹•ï¼ˆstart_page > 1ã®å ´åˆï¼‰
        if start_page > 1:
            log(f"  é–‹å§‹ãƒšãƒ¼ã‚¸ {start_page} ã¾ã§ç§»å‹•ä¸­...")
            for _ in range(start_page - 1):
                try:
                    next_button = page.locator('button.btn-next:not([disabled])')
                    button_count = await next_button.count()

                    if button_count > 0:
                        await next_button.click()
                        await page.wait_for_load_state("networkidle", timeout=30000)
                        await asyncio.sleep(1)
                    else:
                        log(f"[WARN] æ¬¡ã®ãƒšãƒ¼ã‚¸ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        return all_data
                except Exception as e:
                    log(f"[ERROR] ãƒšãƒ¼ã‚¸ç§»å‹•ã‚¨ãƒ©ãƒ¼: {e}")
                    return all_data

            log(f"  â†’ {start_page}ãƒšãƒ¼ã‚¸ç›®ã«åˆ°é”")

        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ«ãƒ¼ãƒ—
        for page_num in range(start_page, end_page + 1):
            log(f"  ãƒšãƒ¼ã‚¸ {page_num}/20 ã‚’å‡¦ç†ä¸­...")

            # å…¨ã¦ã®è¡Œã‚’å±•é–‹
            log(f"    å…¨ã¦ã®è¡Œã‚’å±•é–‹ä¸­...")
            expand_result = await page.evaluate('''() => {
                const expandButtons = document.querySelectorAll('td.el-table__expand-column .el-table__expand-icon');
                let clickedCount = 0;

                expandButtons.forEach(button => {
                    if (!button.classList.contains('el-table__expand-icon--expanded')) {
                        button.click();
                        clickedCount++;
                    }
                });

                return {
                    total: expandButtons.length,
                    clicked: clickedCount
                };
            }''')
            log(f"    â†’ {expand_result['total']}å€‹ä¸­{expand_result['clicked']}å€‹ã®å±•é–‹ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯")

            # DOMæ›´æ–°ã‚’å¾…æ©Ÿ
            await page.wait_for_timeout(3000)

            # ãƒ†ãƒ¼ãƒ–ãƒ«ã®å…¨è¡Œã‚’èµ°æŸ»ã—ã¦ASINã¨ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º
            data_on_page = await page.evaluate('''() => {
                const data = [];
                const rows = Array.from(document.querySelectorAll('table tbody tr'));

                let currentAsin = null;

                rows.forEach((row, index) => {
                    const rowText = row.textContent || '';
                    const asinMatch = rowText.match(/ASIN:\\s*([A-Z0-9]{10})/);

                    if (asinMatch) {
                        currentAsin = asinMatch[1];
                    }

                    const tableExpand = row.querySelector('.table-expand');
                    if (tableExpand && currentAsin) {
                        let categories = [];
                        let nodeIdPaths = '';

                        const productType = tableExpand.querySelector('.product-type');
                        if (productType) {
                            const categoryLinks = productType.querySelectorAll('a.type');

                            categoryLinks.forEach((link, linkIndex) => {
                                const categoryName = link.textContent.trim();
                                if (categoryName) {
                                    categories.push(categoryName);
                                }

                                if (linkIndex === categoryLinks.length - 1 && link.href) {
                                    try {
                                        const url = new URL(link.href, window.location.origin);
                                        const nodeIdPathsParam = url.searchParams.get('nodeIdPaths');
                                        if (nodeIdPathsParam) {
                                            nodeIdPaths = nodeIdPathsParam;
                                        }
                                    } catch (e) {
                                        // URLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                                    }
                                }
                            });
                        }

                        data.push({
                            asin: currentAsin,
                            category: categories.join(' > '),
                            nodeIdPaths: nodeIdPaths
                        });

                        currentAsin = null;
                    }
                });

                return data;
            }''')

            categories_found = sum(1 for item in data_on_page if item.get('category'))
            log(f"    â†’ ã‚«ãƒ†ã‚´ãƒªæƒ…å ±: {categories_found}ä»¶ / {len(data_on_page)}ä»¶")
            log(f"    â†’ {len(data_on_page)}ä»¶æŠ½å‡º")
            all_data.extend(data_on_page)

            # æœ€å¾Œã®ãƒšãƒ¼ã‚¸ã§ãªã„å ´åˆã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã«ç§»å‹•
            if page_num < end_page:
                try:
                    next_button = page.locator('button.btn-next:not([disabled])')
                    button_count = await next_button.count()

                    if button_count > 0:
                        await next_button.click()
                        await page.wait_for_load_state("networkidle", timeout=30000)
                        await asyncio.sleep(1)
                        log(f"    æ¬¡ã®ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ã¾ã—ãŸ")
                    else:
                        log(f"[WARN] æ¬¡ã®ãƒšãƒ¼ã‚¸ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚{page_num}ãƒšãƒ¼ã‚¸ã§çµ‚äº†ã—ã¾ã™ã€‚")
                        break

                except Exception as e:
                    log(f"[WARN] ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
                    log(f"[WARN] {page_num}ãƒšãƒ¼ã‚¸ã¾ã§ã®çµæœã‚’è¿”ã—ã¾ã™")
                    break

        # limitä»¶æ•°ã¾ã§åˆ¶é™
        all_data = all_data[:limit]
        log(f"åˆè¨ˆ {len(all_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆç›®æ¨™: {limit}ä»¶ï¼‰")

        return all_data

    except Exception as e:
        log(f"[ERROR] ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        raise


class CategoryBasedExtractorV2:
    """ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã®ASINæŠ½å‡ºã‚¯ãƒ©ã‚¹ v2ï¼ˆå±¥æ­´ç®¡ç†æ©Ÿèƒ½ä»˜ãï¼‰"""

    def __init__(self, args):
        self.args = args
        self.db_path = project_root / 'sourcing' / 'data' / 'sourcing.db'
        self.history_manager = CategoryHistoryManager(Path(args.history_file))

        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'total_extracted': 0,
            'new_asins': 0,
            'duplicate_asins': 0,
            'categories_processed': 0,
        }

    def log(self, message: str):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")

    async def run(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        self.log("=" * 60)
        self.log("ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ASINè‡ªå‹•æŠ½å‡º v2ï¼ˆå±¥æ­´ç®¡ç†æ©Ÿèƒ½ä»˜ãï¼‰")
        self.log("=" * 60)
        self.log(f"ç›®æ¨™æ–°è¦ASINæ•°: {self.args.target_new_asins}ä»¶")
        self.log(f"ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°: {self.args.pages_per_category}ãƒšãƒ¼ã‚¸")
        self.log(f"å†é–‹ãƒ¢ãƒ¼ãƒ‰: {'ON' if self.args.resume else 'OFF'}")
        self.log(f"å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«: {self.args.history_file}")
        self.log("")

        try:
            async with create_browser_session(headless=False) as (browser, page):
                if self.args.resume:
                    # å†é–‹ãƒ¢ãƒ¼ãƒ‰: å±¥æ­´ã‹ã‚‰ç¶šãã‚’å‡¦ç†
                    await self._resume_extraction(page)
                else:
                    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: æ–°è¦ã«æŠ½å‡º
                    await self._normal_extraction(page)

                # å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
                self.log("")
                self.log("=" * 60)
                self.log("æŠ½å‡ºå®Œäº†")
                self.log("=" * 60)
                self.log(f"æ–°è¦ASINæ•°: {self.stats['new_asins']}ä»¶")
                self.log(f"å‡¦ç†ã‚«ãƒ†ã‚´ãƒªæ•°: {self.stats['categories_processed']}ä»¶")
                self.log(f"ç·æŠ½å‡ºASINæ•°: {self.stats['total_extracted']}ä»¶")
                self.log(f"é‡è¤‡ASINæ•°: {self.stats['duplicate_asins']}ä»¶")
                if self.stats['total_extracted'] > 0:
                    self.log(f"æ–°è¦ç‡: {self.stats['new_asins'] / self.stats['total_extracted'] * 100:.1f}%")

        except KeyboardInterrupt:
            self.log("")
            self.log("[WARN] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
            sys.exit(130)

        except Exception as e:
            self.log("")
            self.log("[ERROR] ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            self.log(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

    async def _resume_extraction(self, page):
        """å†é–‹ãƒ¢ãƒ¼ãƒ‰: å±¥æ­´ã‹ã‚‰ç¶šãã‚’å‡¦ç†"""
        self.log("ã€å†é–‹ãƒ¢ãƒ¼ãƒ‰ã€‘å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‡¦ç†ã‚’å†é–‹ã—ã¾ã™")

        categories_to_process = self.history_manager.get_categories_to_process(
            self.args.pages_per_category
        )

        if not categories_to_process:
            self.log("[WARN] å‡¦ç†ã™ã¹ãã‚«ãƒ†ã‚´ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            self.log("[INFO] å…¨ã‚«ãƒ†ã‚´ãƒªãŒæŒ‡å®šãƒšãƒ¼ã‚¸æ•°ã¾ã§å‡¦ç†æ¸ˆã¿ã§ã™")
            return

        self.log(f"  å‡¦ç†å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªæ•°: {len(categories_to_process)}ä»¶")

        all_new_asins = set()

        for i, (category_name, category_info, start_page) in enumerate(categories_to_process):
            if len(all_new_asins) >= self.args.target_new_asins:
                self.log(f"[OK] ç›®æ¨™é”æˆ: {len(all_new_asins)}ä»¶ã®æ–°è¦ASIN")
                break

            self.log("")
            self.log(f"[ã‚«ãƒ†ã‚´ãƒª {i+1}/{len(categories_to_process)}]")
            self.log(f"  ã‚«ãƒ†ã‚´ãƒª: {category_name}")
            self.log(f"  nodeIdPaths: {category_info['nodeIdPaths']}")
            self.log(f"  å–å¾—æ¸ˆã¿ãƒšãƒ¼ã‚¸: {category_info.get('pages_extracted', 0)}ãƒšãƒ¼ã‚¸")
            self.log(f"  é–‹å§‹ãƒšãƒ¼ã‚¸: {start_page}ãƒšãƒ¼ã‚¸")

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ½å‡º
            new_asins = await self._extract_by_category_paginated(
                page,
                category_name,
                category_info['nodeIdPaths'],
                start_page,
                all_new_asins
            )

            all_new_asins.update(new_asins)
            self.stats['categories_processed'] += 1

            self.log(f"  â†’ ç´¯è¨ˆæ–°è¦ASIN: {len(all_new_asins)}ä»¶ / {self.args.target_new_asins}ä»¶")

        # çµæœä¿å­˜
        await self._save_results(all_new_asins, categories_to_process)

    async def _normal_extraction(self, page):
        """é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: æ–°è¦ã«æŠ½å‡º"""
        self.log("ã€é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã€‘æ–°è¦ã«ã‚«ãƒ†ã‚´ãƒªã‚’æ¢ç´¢ã—ã¦æŠ½å‡ºã—ã¾ã™")

        # ã‚¹ãƒ†ãƒƒãƒ—1: åˆæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘åˆæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’é–‹å§‹...")
        sample_data = await self._initial_sampling(page)

        if not sample_data:
            self.log("[ERROR] ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return

        # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆ
        self.log("")
        self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘ã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆã‚’åˆ†æä¸­...")
        category_stats = self._analyze_categories(sample_data)

        if not category_stats:
            self.log("[ERROR] ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return

        # ã‚¹ãƒ†ãƒƒãƒ—3: æ—¢å­˜DBã¨æ¯”è¼ƒ
        self.log("")
        self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘æ—¢å­˜DBã®ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã‚’ç¢ºèªä¸­...")
        existing_categories = self._get_existing_categories()
        unexplored_categories = self._identify_unexplored_categories(
            category_stats, existing_categories
        )

        # ã‚¹ãƒ†ãƒƒãƒ—4: å„ªå…ˆé †ä½ä»˜ã‘
        self.log("")
        self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘ã‚«ãƒ†ã‚´ãƒªã‚’å„ªå…ˆé †ä½ä»˜ã‘ä¸­...")
        prioritized_categories = self._prioritize_categories(
            category_stats, unexplored_categories
        )

        if not prioritized_categories:
            self.log("[WARN] å„ªå…ˆã™ã¹ãã‚«ãƒ†ã‚´ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

        # ã‚¹ãƒ†ãƒƒãƒ—5: ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ½å‡ºãƒ«ãƒ¼ãƒ—
        self.log("")
        self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—5ã€‘ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ½å‡ºã‚’é–‹å§‹...")
        all_new_asins = set()

        for i, (category_name, category_info) in enumerate(prioritized_categories):
            if len(all_new_asins) >= self.args.target_new_asins:
                self.log(f"[OK] ç›®æ¨™é”æˆ: {len(all_new_asins)}ä»¶ã®æ–°è¦ASIN")
                break

            self.log("")
            self.log(f"[ã‚«ãƒ†ã‚´ãƒª {i+1}/{len(prioritized_categories)}]")
            self.log(f"  ã‚«ãƒ†ã‚´ãƒª: {category_name}")
            self.log(f"  nodeIdPaths: {category_info['nodeIdPaths']}")

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ½å‡ºï¼ˆ1ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰ï¼‰
            new_asins = await self._extract_by_category_paginated(
                page,
                category_name,
                category_info['nodeIdPaths'],
                start_page=1,
                already_found_asins=all_new_asins
            )

            all_new_asins.update(new_asins)
            self.stats['categories_processed'] += 1

            self.log(f"  â†’ ç´¯è¨ˆæ–°è¦ASIN: {len(all_new_asins)}ä»¶ / {self.args.target_new_asins}ä»¶")

        # çµæœä¿å­˜
        await self._save_results(all_new_asins, prioritized_categories)

    async def _initial_sampling(self, page) -> List[Dict]:
        """åˆæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        self.log(f"  ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {self.args.sample_size}ä»¶")

        if self.args.use_intermediate_categories:
            self.log(f"  ä¸­é–“ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºãƒ¢ãƒ¼ãƒ‰: ONï¼ˆå…¨éšå±¤ã‚’æŠ½å‡ºï¼‰")
        else:
            self.log(f"  ä¸­é–“ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºãƒ¢ãƒ¼ãƒ‰: OFFï¼ˆæœ€æ·±ã‚«ãƒ†ã‚´ãƒªã®ã¿ï¼‰")

        url = build_product_research_url(
            market=self.args.market,
            sales_min=self.args.sales_min,
            price_min=self.args.price_min,
            amz=True,
            fba=True
        )

        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
        await page.wait_for_timeout(5000)

        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆãƒ•ãƒ©ã‚°ã«å¿œã˜ã¦é–¢æ•°ã‚’é¸æŠï¼‰
        if self.args.use_intermediate_categories:
            from sourcing.sources.sellersprite.utils.category_extractor_v2 import extract_asins_with_all_category_levels
            data = await extract_asins_with_all_category_levels(page, self.args.sample_size)

            # v2ã®ãƒ‡ãƒ¼ã‚¿ã¯è¤‡æ•°ã‚¨ãƒ³ãƒˆãƒªï¼ˆASINÃ—ã‚«ãƒ†ã‚´ãƒªãƒ¬ãƒ™ãƒ«ï¼‰ã‚’è¿”ã™ãŸã‚ã€çµ±è¨ˆã‚’è¨ˆç®—
            unique_asins = len(set(item['asin'] for item in data))
            self.log(f"  â†’ {len(data)}ä»¶ã®ã‚«ãƒ†ã‚´ãƒªã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ASIN: {unique_asins}ä»¶ï¼‰")
        else:
            from sourcing.sources.sellersprite.utils.category_extractor import extract_asins_with_categories
            data = await extract_asins_with_categories(page, self.args.sample_size)

            self.log(f"  â†’ {len(data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")

            categories_found = sum(1 for item in data if item.get('category'))
            self.log(f"  â†’ ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚ã‚Š: {categories_found}ä»¶ / {len(data)}ä»¶")

        return data

    def _analyze_categories(self, data: List[Dict]) -> Dict[str, Dict]:
        """ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®çµ±è¨ˆæƒ…å ±ã‚’åˆ†æ"""
        category_stats = {}

        for item in data:
            category = item.get('category', '').strip()
            node_id_paths = item.get('nodeIdPaths', '').strip()

            if not category:
                continue

            if category not in category_stats:
                category_stats[category] = {
                    'count': 0,
                    'nodeIdPaths': node_id_paths
                }

            category_stats[category]['count'] += 1

        sorted_stats = dict(
            sorted(category_stats.items(), key=lambda x: x[1]['count'], reverse=True)
        )

        self.log(f"  ç™ºè¦‹ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªæ•°: {len(sorted_stats)}ä»¶")
        self.log(f"  ãƒˆãƒƒãƒ—5ã‚«ãƒ†ã‚´ãƒª:")
        for i, (category, info) in enumerate(list(sorted_stats.items())[:5]):
            self.log(f"    {i+1}. {category}: {info['count']}ä»¶")

        return sorted_stats

    def _get_existing_categories(self) -> Dict[str, int]:
        """æ—¢å­˜DBã®ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            cursor.execute('''
                SELECT category, COUNT(*) as count
                FROM sourcing_candidates
                WHERE category IS NOT NULL AND category != ''
                GROUP BY category
                ORDER BY count DESC
            ''')

            rows = cursor.fetchall()
            existing_categories = {row[0]: row[1] for row in rows}

            self.log(f"  æ—¢å­˜DBå†…ã®ã‚«ãƒ†ã‚´ãƒªæ•°: {len(existing_categories)}ä»¶")

            return existing_categories

        finally:
            conn.close()

    def _identify_unexplored_categories(
        self,
        category_stats: Dict[str, Dict],
        existing_categories: Dict[str, int]
    ) -> Set[str]:
        """æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®š"""
        unexplored = set()

        for category in category_stats.keys():
            if category not in existing_categories:
                unexplored.add(category)

        self.log(f"  æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªæ•°: {len(unexplored)}ä»¶")

        return unexplored

    def _prioritize_categories(
        self,
        category_stats: Dict[str, Dict],
        unexplored_categories: Set[str]
    ) -> List[tuple]:
        """ã‚«ãƒ†ã‚´ãƒªã‚’å„ªå…ˆé †ä½ä»˜ã‘"""
        unexplored_list = [
            (cat, info) for cat, info in category_stats.items()
            if cat in unexplored_categories
        ]

        explored_list = [
            (cat, info) for cat, info in category_stats.items()
            if cat not in unexplored_categories
        ]

        prioritized = unexplored_list + explored_list
        prioritized = prioritized[:self.args.max_categories]

        self.log(f"  å„ªå…ˆé †ä½ä»˜ã‘å®Œäº†: {len(prioritized)}ã‚«ãƒ†ã‚´ãƒªã‚’å‡¦ç†å¯¾è±¡ã«")

        return prioritized

    async def _extract_by_category_paginated(
        self,
        page,
        category_name: str,
        node_id_paths: str,
        start_page: int,
        already_found_asins: Set[str]
    ) -> Set[str]:
        """ç‰¹å®šã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ASINã‚’æŠ½å‡ºï¼ˆãƒšãƒ¼ã‚¸æŒ‡å®šå¯èƒ½ï¼‰"""
        if not node_id_paths:
            self.log(f"  [WARN] nodeIdPathsãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            return set()

        try:
            url = build_product_research_url(
                market=self.args.market,
                sales_min=self.args.sales_min,
                price_min=self.args.price_min,
                amz=True,
                fba=True,
                node_id_paths=node_id_paths
            )

            await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            await page.wait_for_timeout(5000)

            # ãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—
            items_per_page = 100
            pages_to_extract = self.args.pages_per_category - (start_page - 1)
            limit = min(pages_to_extract * items_per_page, 2000)  # æœ€å¤§2000ä»¶

            # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            data = await extract_asins_with_categories_paginated(page, limit, start_page)

            asins = {item['asin'] for item in data if item.get('asin')}
            self.stats['total_extracted'] += len(asins)

            # æ—¢å­˜DBã®ASINã‚’å–å¾—
            existing_asins = self._get_existing_asins()

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            new_asins = asins - existing_asins - already_found_asins

            duplicate_count = len(asins) - len(new_asins)
            self.stats['duplicate_asins'] += duplicate_count
            self.stats['new_asins'] += len(new_asins)

            self.log(f"  å–å¾—: {len(asins)}ä»¶")
            self.log(f"  æ–°è¦: {len(new_asins)}ä»¶ ({len(new_asins)/max(len(asins), 1)*100:.1f}%)")
            self.log(f"  é‡è¤‡: {duplicate_count}ä»¶")

            # å±¥æ­´ã‚’æ›´æ–°
            self.history_manager.update_category(
                category_name,
                node_id_paths,
                start_page + pages_to_extract - 1,
                len(new_asins)
            )
            self.history_manager.save_history()

            return new_asins

        except Exception as e:
            self.log(f"  [ERROR] ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return set()

    def _get_existing_asins(self) -> Set[str]:
        """æ—¢å­˜DBã®ASINã‚»ãƒƒãƒˆã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            cursor.execute('SELECT asin FROM sourcing_candidates')
            rows = cursor.fetchall()
            return {row[0] for row in rows}

        finally:
            conn.close()

    async def _save_results(self, new_asins: Set[str], categories: List[tuple]):
        """çµæœã‚’ä¿å­˜"""
        if self.args.output:
            output_path = Path(self.args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            with output_path.open('w', encoding='utf-8') as f:
                for asin in sorted(new_asins):
                    f.write(f"{asin}\n")

            self.log(f"  ASINãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {output_path}")
            self.log(f"  ä¿å­˜ä»¶æ•°: {len(new_asins)}ä»¶")

        if self.args.report:
            await self._generate_report(new_asins, categories)

        # å±¥æ­´ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        self.history_manager.update_metadata(len(new_asins))
        self.history_manager.save_history()
        self.log(f"  å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {self.args.history_file}")

    async def _generate_report(self, new_asins: Set[str], categories: List[tuple]):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report_path = Path(self.args.report)
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with report_path.open('w', encoding='utf-8') as f:
            f.write(f"# ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ASINè‡ªå‹•æŠ½å‡ºãƒ¬ãƒãƒ¼ãƒˆ v2\n\n")
            f.write(f"**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write(f"## ğŸ“Š æŠ½å‡ºçµæœã‚µãƒãƒªãƒ¼\n\n")
            f.write(f"| æŒ‡æ¨™ | å€¤ |\n")
            f.write(f"|------|------|\n")
            f.write(f"| æ–°è¦ASINæ•° | {len(new_asins)}ä»¶ |\n")
            f.write(f"| ç·æŠ½å‡ºASINæ•° | {self.stats['total_extracted']}ä»¶ |\n")
            f.write(f"| é‡è¤‡ASINæ•° | {self.stats['duplicate_asins']}ä»¶ |\n")
            if self.stats['total_extracted'] > 0:
                f.write(f"| æ–°è¦ç‡ | {len(new_asins) / self.stats['total_extracted'] * 100:.1f}% |\n")
            f.write(f"| å‡¦ç†ã‚«ãƒ†ã‚´ãƒªæ•° | {self.stats['categories_processed']}ä»¶ |\n")
            f.write(f"\n")

            f.write(f"## ğŸ“‚ å‡¦ç†ã‚«ãƒ†ã‚´ãƒªä¸€è¦§\n\n")
            for i, item in enumerate(categories[:self.stats['categories_processed']]):
                if len(item) == 3:  # å†é–‹ãƒ¢ãƒ¼ãƒ‰
                    category, info, start_page = item
                    f.write(f"{i+1}. **{category}**\n")
                    f.write(f"   - nodeIdPaths: `{info['nodeIdPaths']}`\n")
                    f.write(f"   - é–‹å§‹ãƒšãƒ¼ã‚¸: {start_page}ãƒšãƒ¼ã‚¸\n")
                else:  # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰
                    category, info = item
                    f.write(f"{i+1}. **{category}**\n")
                    f.write(f"   - ã‚µãƒ³ãƒ—ãƒ«å†…å•†å“æ•°: {info['count']}ä»¶\n")
                    f.write(f"   - nodeIdPaths: `{info['nodeIdPaths']}`\n")
                f.write(f"\n")

            f.write(f"## âš™ï¸ å®Ÿè¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n\n")
            f.write(f"```\n")
            f.write(f"ç›®æ¨™æ–°è¦ASINæ•°: {self.args.target_new_asins}ä»¶\n")
            f.write(f"ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°: {self.args.pages_per_category}ãƒšãƒ¼ã‚¸\n")
            f.write(f"å†é–‹ãƒ¢ãƒ¼ãƒ‰: {'ON' if self.args.resume else 'OFF'}\n")
            f.write(f"å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«: {self.args.history_file}\n")
            f.write(f"è²©å£²æ•°ç¯„å›²: {self.args.sales_min} ä»¥ä¸Š\n")
            f.write(f"ä¾¡æ ¼ç¯„å›²: {self.args.price_min} ä»¥ä¸Š\n")
            f.write(f"å¸‚å ´: {self.args.market}\n")
            f.write(f"```\n")

        self.log(f"  ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="SellerSprite ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ASINè‡ªå‹•æŠ½å‡º v2ï¼ˆå±¥æ­´ç®¡ç†æ©Ÿèƒ½ä»˜ãï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # åˆå›å®Ÿè¡Œï¼ˆå±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼‰
  python sourcing/scripts/auto_extract_by_categories_v2.py \\
    --target-new-asins 3000 \\
    --pages-per-category 10 \\
    --history-file category_history.json \\
    --output asins_batch1.txt \\
    --report report_batch1.md

  # å‰å›ã®ç¶šãã‹ã‚‰å®Ÿè¡Œï¼ˆ11ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰20ãƒšãƒ¼ã‚¸ç›®ã¾ã§ï¼‰
  python sourcing/scripts/auto_extract_by_categories_v2.py \\
    --target-new-asins 3000 \\
    --resume \\
    --history-file category_history.json \\
    --pages-per-category 20 \\
    --output asins_batch2.txt \\
    --report report_batch2.md
        """
    )

    # ç›®æ¨™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--target-new-asins",
        type=int,
        default=3000,
        help="ç›®æ¨™æ–°è¦ASINæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3000ï¼‰"
    )

    # ãƒšãƒ¼ã‚¸ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--pages-per-category",
        type=int,
        default=10,
        help="å„ã‚«ãƒ†ã‚´ãƒªã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆ1-20ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰"
    )

    # å±¥æ­´ç®¡ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--resume",
        action="store_true",
        help="å‰å›ã®ç¶šãã‹ã‚‰å®Ÿè¡Œï¼ˆå±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å†é–‹ï¼‰"
    )
    parser.add_argument(
        "--history-file",
        type=str,
        required=True,
        help="å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆJSONå½¢å¼ï¼‰"
    )

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã®ã¿ï¼‰
    parser.add_argument(
        "--sample-size",
        type=int,
        default=1000,
        help="åˆæœŸã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ã€æœ€å¤§: 2000ï¼‰"
    )
    parser.add_argument(
        "--max-categories",
        type=int,
        default=30,
        help="æœ€å¤§ã‚«ãƒ†ã‚´ãƒªæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30ï¼‰"
    )

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--sales-min",
        type=int,
        default=300,
        help="æœˆé–“è²©å£²æ•°ã®æœ€å°å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300ï¼‰"
    )
    parser.add_argument(
        "--price-min",
        type=int,
        default=2500,
        help="ä¾¡æ ¼ã®æœ€å°å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2500ï¼‰"
    )
    parser.add_argument(
        "--market",
        type=str,
        default="JP",
        help="å¸‚å ´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: JPï¼‰"
    )

    # å‡ºåŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--output",
        type=str,
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆASINä¸€è¦§ï¼‰"
    )
    parser.add_argument(
        "--report",
        type=str,
        help="ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆMarkdownå½¢å¼ï¼‰"
    )

    # ä¸­é–“ã‚«ãƒ†ã‚´ãƒªæ©Ÿèƒ½ï¼ˆå®Ÿé¨“çš„ï¼‰
    parser.add_argument(
        "--use-intermediate-categories",
        action="store_true",
        help="ä¸­é–“ã‚«ãƒ†ã‚´ãƒªã‚‚æŠ½å‡ºå¯¾è±¡ã«å«ã‚ã‚‹ï¼ˆä¾‹ï¼šã€ŒA > B > C > Dã€ã‹ã‚‰å…¨éšå±¤ã‚’æŠ½å‡ºï¼‰"
    )

    args = parser.parse_args()

    # å®Ÿè¡Œ
    extractor = CategoryBasedExtractorV2(args)
    await extractor.run()


if __name__ == "__main__":
    asyncio.run(main())
