"""
SellerSprite ã‚«ãƒ†ã‚´ãƒªåˆ¥ASINè‡ªå‹•æŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã‚«ãƒ†ã‚´ãƒªè»¸ã§ASINã‚’è‡ªå‹•æŠ½å‡ºã™ã‚‹ã“ã¨ã§ã€æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®é‡è¤‡ã‚’æœ€å°åŒ–ã—ã€
æ–°è¦ASINã‚’åŠ¹ç‡çš„ã«å–å¾—ã™ã‚‹ã€‚

ä½¿ç”¨ä¾‹:
    # åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ï¼ˆç›®æ¨™10,000ä»¶ã®æ–°è¦ASINï¼‰
    python sourcing/scripts/auto_extract_by_categories.py \
      --target-new-asins 10000 \
      --sample-size 1000 \
      --asins-per-category 2000 \
      --sales-min 300 \
      --price-min 2500

    # ã‚«ãƒ†ã‚´ãƒªæ•°ã‚’åˆ¶é™ã—ã¦å®Ÿè¡Œ
    python sourcing/scripts/auto_extract_by_categories.py \
      --target-new-asins 5000 \
      --max-categories 10 \
      --output category_asins_20251126.txt

ãƒ•ãƒ­ãƒ¼:
    1. åˆæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ500-1,000ä»¶ï¼‰ã§ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—
    2. ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®å•†å“æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    3. æ—¢å­˜DBã®ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã¨æ¯”è¼ƒã—ã¦æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®š
    4. æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªã‚’å„ªå…ˆé †ä½ä»˜ã‘
    5. å„ã‚«ãƒ†ã‚´ãƒªã§2,000ä»¶ãšã¤æŠ½å‡ºï¼ˆnodeIdPathsã‚’ä½¿ç”¨ï¼‰
    6. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
    7. ç›®æ¨™ä»¶æ•°ã«é”ã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—
"""

import argparse
import asyncio
import sys
import sqlite3
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set
from collections import Counter
from dotenv import load_dotenv

# ecautoãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
env_path = project_root / 'sourcing' / 'sources' / 'sellersprite' / '.env'
load_dotenv(dotenv_path=env_path)

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªå®Ÿè£…ã‚’ä½¿ç”¨ï¼‰
from sourcing.sources.sellersprite.utils.category_extractor import (
    build_product_research_url,
    extract_asins_with_categories,
    create_browser_session
)


class CategoryBasedExtractor:
    """ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã®ASINæŠ½å‡ºã‚¯ãƒ©ã‚¹"""

    def __init__(self, args):
        """
        Args:
            args: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
        """
        self.args = args
        self.db_path = project_root / 'sourcing' / 'data' / 'sourcing.db'

        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'total_extracted': 0,
            'new_asins': 0,
            'duplicate_asins': 0,
            'categories_processed': 0,
        }

    def log(self, message: str):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")

    async def run(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        self.log("=" * 60)
        self.log("ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ASINè‡ªå‹•æŠ½å‡ºã‚’é–‹å§‹")
        self.log("=" * 60)
        self.log(f"ç›®æ¨™æ–°è¦ASINæ•°: {self.args.target_new_asins}ä»¶")
        self.log(f"åˆæœŸã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {self.args.sample_size}ä»¶")
        self.log(f"ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šã®å–å¾—æ•°: {self.args.asins_per_category}ä»¶")
        self.log(f"è²©å£²æ•°ç¯„å›²: {self.args.sales_min} ä»¥ä¸Š")
        self.log(f"ä¾¡æ ¼ç¯„å›²: {self.args.price_min} ä»¥ä¸Š")
        self.log("")

        try:
            # ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’1å›ã ã‘ä½œæˆï¼ˆå…¨ã¦ã®å‡¦ç†ã§å†åˆ©ç”¨ï¼‰
            async with create_browser_session(headless=False) as (browser, page):
                # ã‚¹ãƒ†ãƒƒãƒ—1: åˆæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘åˆæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’é–‹å§‹...")
                sample_data = await self._initial_sampling(page)

                if not sample_data:
                    self.log("[ERROR] ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    return

                # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆ
                self.log("")
                self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘ã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆã‚’åˆ†æä¸­...")
                category_stats = self._analyze_categories(sample_data)

                if not category_stats:
                    self.log("[ERROR] ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    return

                # ã‚¹ãƒ†ãƒƒãƒ—3: æ—¢å­˜DBã¨æ¯”è¼ƒ
                self.log("")
                self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘æ—¢å­˜DBã®ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã‚’ç¢ºèªä¸­...")
                existing_categories = self._get_existing_categories()
                unexplored_categories = self._identify_unexplored_categories(
                    category_stats, existing_categories
                )

                # ã‚¹ãƒ†ãƒƒãƒ—4: å„ªå…ˆé †ä½ä»˜ã‘
                self.log("")
                self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘ã‚«ãƒ†ã‚´ãƒªã‚’å„ªå…ˆé †ä½ä»˜ã‘ä¸­...")
                prioritized_categories = self._prioritize_categories(
                    category_stats, unexplored_categories
                )

                if not prioritized_categories:
                    self.log("[WARN] å„ªå…ˆã™ã¹ãã‚«ãƒ†ã‚´ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    self.log("[INFO] å…¨ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æŠ½å‡ºã‚’è©¦ã¿ã¾ã™")
                    prioritized_categories = list(category_stats.items())[:self.args.max_categories]

                # ã‚¹ãƒ†ãƒƒãƒ—5: ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ½å‡ºãƒ«ãƒ¼ãƒ—
                self.log("")
                self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—5ã€‘ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ½å‡ºã‚’é–‹å§‹...")
                all_new_asins = set()

                for i, (category_name, category_info) in enumerate(prioritized_categories):
                    if len(all_new_asins) >= self.args.target_new_asins:
                        self.log(f"[OK] ç›®æ¨™é”æˆ: {len(all_new_asins)}ä»¶ã®æ–°è¦ASIN")
                        break

                    self.log("")
                    self.log(f"[ã‚«ãƒ†ã‚´ãƒª {i+1}/{len(prioritized_categories)}]")
                    self.log(f"  ã‚«ãƒ†ã‚´ãƒª: {category_name}")
                    self.log(f"  ã‚µãƒ³ãƒ—ãƒ«å†…å•†å“æ•°: {category_info['count']}ä»¶")
                    self.log(f"  nodeIdPaths: {category_info['nodeIdPaths']}")

                    # ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ½å‡º
                    new_asins = await self._extract_by_category(
                        page,
                        category_name,
                        category_info['nodeIdPaths'],
                        all_new_asins
                    )

                    all_new_asins.update(new_asins)
                    self.stats['categories_processed'] += 1

                    self.log(f"  â†’ ç´¯è¨ˆæ–°è¦ASIN: {len(all_new_asins)}ä»¶ / {self.args.target_new_asins}ä»¶")

                # ã‚¹ãƒ†ãƒƒãƒ—6: çµæœä¿å­˜
                self.log("")
                self.log("ã€ã‚¹ãƒ†ãƒƒãƒ—6ã€‘çµæœã‚’ä¿å­˜ä¸­...")
                await self._save_results(all_new_asins, prioritized_categories)

                # å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
                self.log("")
                self.log("=" * 60)
                self.log("æŠ½å‡ºå®Œäº†")
                self.log("=" * 60)
                self.log(f"æ–°è¦ASINæ•°: {len(all_new_asins)}ä»¶")
                self.log(f"å‡¦ç†ã‚«ãƒ†ã‚´ãƒªæ•°: {self.stats['categories_processed']}ä»¶")
                self.log(f"ç·æŠ½å‡ºASINæ•°: {self.stats['total_extracted']}ä»¶")
                self.log(f"é‡è¤‡ASINæ•°: {self.stats['duplicate_asins']}ä»¶")
                if self.stats['total_extracted'] > 0:
                    self.log(f"æ–°è¦ç‡: {len(all_new_asins) / self.stats['total_extracted'] * 100:.1f}%")

        except KeyboardInterrupt:
            self.log("")
            self.log("[WARN] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
            sys.exit(130)

        except Exception as e:
            self.log("")
            self.log("[ERROR] ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            self.log(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

    async def _initial_sampling(self, page) -> List[Dict]:
        """
        åˆæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ä»˜ãã§ASINã‚’å–å¾—

        å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆcategory_extractorï¼‰ã®ã‚¯ãƒªãƒ¼ãƒ³ãªå®Ÿè£…ã‚’ä½¿ç”¨

        Args:
            page: Playwrightãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

        Returns:
            [{"asin": "B00XXX", "category": "Home & Kitchen", "nodeIdPaths": "[...]"}, ...]
        """
        self.log(f"  ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {self.args.sample_size}ä»¶")

        # URLã‚’æ§‹ç¯‰
        url = build_product_research_url(
            market=self.args.market,
            sales_min=self.args.sales_min,
            price_min=self.args.price_min,
            amz=True,
            fba=True
        )

        # ãƒšãƒ¼ã‚¸ã«é·ç§»
        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
        await page.wait_for_timeout(5000)

        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªå®Ÿè£…ã‚’ä½¿ç”¨ï¼‰
        data = await extract_asins_with_categories(page, self.args.sample_size)

        self.log(f"  â†’ {len(data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")

        # ãƒ‡ãƒãƒƒã‚°: ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ãŒå–å¾—ã§ãã¦ã„ã‚‹ã‹ç¢ºèª
        categories_found = sum(1 for item in data if item.get('category'))
        self.log(f"  â†’ ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚ã‚Š: {categories_found}ä»¶ / {len(data)}ä»¶")

        return data

    def _analyze_categories(self, data: List[Dict]) -> Dict[str, Dict]:
        """
        ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®çµ±è¨ˆæƒ…å ±ã‚’åˆ†æ

        Args:
            data: [{"asin": "B00XXX", "category": "...", "nodeIdPaths": "..."}, ...]

        Returns:
            {
                "Home & Kitchen": {"count": 50, "nodeIdPaths": "[...]"},
                "Beauty": {"count": 30, "nodeIdPaths": "[...]"},
                ...
            }
        """
        category_stats = {}

        for item in data:
            category = item.get('category', '').strip()
            node_id_paths = item.get('nodeIdPaths', '').strip()

            if not category:
                continue

            if category not in category_stats:
                category_stats[category] = {
                    'count': 0,
                    'nodeIdPaths': node_id_paths
                }

            category_stats[category]['count'] += 1

        # ã‚«ã‚¦ãƒ³ãƒˆé †ã«ã‚½ãƒ¼ãƒˆ
        sorted_stats = dict(
            sorted(category_stats.items(), key=lambda x: x[1]['count'], reverse=True)
        )

        self.log(f"  ç™ºè¦‹ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªæ•°: {len(sorted_stats)}ä»¶")
        self.log(f"  ãƒˆãƒƒãƒ—5ã‚«ãƒ†ã‚´ãƒª:")
        for i, (category, info) in enumerate(list(sorted_stats.items())[:5]):
            self.log(f"    {i+1}. {category}: {info['count']}ä»¶")

        return sorted_stats

    def _get_existing_categories(self) -> Dict[str, int]:
        """
        æ—¢å­˜DBã®ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã‚’å–å¾—

        Returns:
            {"Home & Kitchen": 1200, "Beauty": 800, ...}
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            cursor.execute('''
                SELECT category, COUNT(*) as count
                FROM sourcing_candidates
                WHERE category IS NOT NULL AND category != ''
                GROUP BY category
                ORDER BY count DESC
            ''')

            rows = cursor.fetchall()
            existing_categories = {row[0]: row[1] for row in rows}

            self.log(f"  æ—¢å­˜DBå†…ã®ã‚«ãƒ†ã‚´ãƒªæ•°: {len(existing_categories)}ä»¶")
            if existing_categories:
                top_existing = list(existing_categories.items())[:3]
                self.log(f"  æ—¢å­˜ãƒˆãƒƒãƒ—3ã‚«ãƒ†ã‚´ãƒª:")
                for category, count in top_existing:
                    self.log(f"    - {category}: {count}ä»¶")

            return existing_categories

        finally:
            conn.close()

    def _identify_unexplored_categories(
        self,
        category_stats: Dict[str, Dict],
        existing_categories: Dict[str, int]
    ) -> Set[str]:
        """
        æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®š

        Args:
            category_stats: ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆ
            existing_categories: æ—¢å­˜DBã®ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ

        Returns:
            æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªã®ã‚»ãƒƒãƒˆ
        """
        unexplored = set()

        for category in category_stats.keys():
            if category not in existing_categories:
                unexplored.add(category)

        self.log(f"  æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªæ•°: {len(unexplored)}ä»¶")

        if unexplored:
            self.log(f"  æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªä¾‹:")
            for i, category in enumerate(list(unexplored)[:5]):
                self.log(f"    - {category}")

        return unexplored

    def _prioritize_categories(
        self,
        category_stats: Dict[str, Dict],
        unexplored_categories: Set[str]
    ) -> List[tuple]:
        """
        ã‚«ãƒ†ã‚´ãƒªã‚’å„ªå…ˆé †ä½ä»˜ã‘

        å„ªå…ˆé †ä½:
        1. æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªï¼ˆå•†å“æ•°ãŒå¤šã„é †ï¼‰
        2. æ—¢å­˜ã‚«ãƒ†ã‚´ãƒªï¼ˆå•†å“æ•°ãŒå¤šã„é †ï¼‰

        Args:
            category_stats: ã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆ
            unexplored_categories: æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªã‚»ãƒƒãƒˆ

        Returns:
            [(category_name, category_info), ...]
        """
        # æœªé–‹æ‹“ã‚«ãƒ†ã‚´ãƒªã‚’å„ªå…ˆ
        unexplored_list = [
            (cat, info) for cat, info in category_stats.items()
            if cat in unexplored_categories
        ]

        # æ—¢å­˜ã‚«ãƒ†ã‚´ãƒª
        explored_list = [
            (cat, info) for cat, info in category_stats.items()
            if cat not in unexplored_categories
        ]

        # çµåˆï¼ˆæœªé–‹æ‹“ â†’ æ—¢å­˜ã®é †ï¼‰
        prioritized = unexplored_list + explored_list

        # max_categories ã¾ã§åˆ¶é™
        prioritized = prioritized[:self.args.max_categories]

        self.log(f"  å„ªå…ˆé †ä½ä»˜ã‘å®Œäº†: {len(prioritized)}ã‚«ãƒ†ã‚´ãƒªã‚’å‡¦ç†å¯¾è±¡ã«")
        self.log(f"  å†…è¨³: æœªé–‹æ‹“={len(unexplored_list[:self.args.max_categories])}ä»¶, "
                 f"æ—¢å­˜={len(prioritized) - len(unexplored_list[:self.args.max_categories])}ä»¶")

        return prioritized

    async def _extract_by_category(
        self,
        page,
        category_name: str,
        node_id_paths: str,
        already_found_asins: Set[str]
    ) -> Set[str]:
        """
        ç‰¹å®šã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ASINã‚’æŠ½å‡º

        å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆcategory_extractorï¼‰ã®ã‚¯ãƒªãƒ¼ãƒ³ãªå®Ÿè£…ã‚’ä½¿ç”¨

        Args:
            page: Playwrightãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            category_name: ã‚«ãƒ†ã‚´ãƒªå
            node_id_paths: nodeIdPathsï¼ˆä¾‹: '["3760911:11060451"]'ï¼‰
            already_found_asins: æ—¢ã«è¦‹ã¤ã‹ã£ã¦ã„ã‚‹ASINã‚»ãƒƒãƒˆï¼ˆä»Šå›ã®å®Ÿè¡Œã§ï¼‰

        Returns:
            æ–°è¦ASINã®ã‚»ãƒƒãƒˆ
        """
        if not node_id_paths:
            self.log(f"  [WARN] nodeIdPathsãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            return set()

        try:
            # URLã‚’æ§‹ç¯‰ï¼ˆnodeIdPathsã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼‰
            url = build_product_research_url(
                market=self.args.market,
                sales_min=self.args.sales_min,
                price_min=self.args.price_min,
                amz=True,
                fba=True,
                node_id_paths=node_id_paths
            )

            # ãƒšãƒ¼ã‚¸ã«é·ç§»
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            await page.wait_for_timeout(5000)

            # ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã¯ä¸è¦ãªã®ã§é«˜é€Ÿï¼‰
            data = await extract_asins_with_categories(page, self.args.asins_per_category)

            # ASINã®ã¿æŠ½å‡º
            asins = {item['asin'] for item in data if item.get('asin')}

            self.stats['total_extracted'] += len(asins)

            # æ—¢å­˜DBã®ASINã‚’å–å¾—
            existing_asins = self._get_existing_asins()

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            new_asins = asins - existing_asins - already_found_asins

            duplicate_count = len(asins) - len(new_asins)
            self.stats['duplicate_asins'] += duplicate_count
            self.stats['new_asins'] += len(new_asins)

            self.log(f"  å–å¾—: {len(asins)}ä»¶")
            self.log(f"  æ–°è¦: {len(new_asins)}ä»¶ ({len(new_asins)/max(len(asins), 1)*100:.1f}%)")
            self.log(f"  é‡è¤‡: {duplicate_count}ä»¶")

            return new_asins

        except Exception as e:
            self.log(f"  [ERROR] ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return set()

    def _get_existing_asins(self) -> Set[str]:
        """
        æ—¢å­˜DBã®ASINã‚»ãƒƒãƒˆã‚’å–å¾—

        Returns:
            æ—¢å­˜ASINã®ã‚»ãƒƒãƒˆ
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            cursor.execute('SELECT asin FROM sourcing_candidates')
            rows = cursor.fetchall()
            return {row[0] for row in rows}

        finally:
            conn.close()

    async def _save_results(self, new_asins: Set[str], categories: List[tuple]):
        """
        çµæœã‚’ä¿å­˜

        Args:
            new_asins: æ–°è¦ASINã‚»ãƒƒãƒˆ
            categories: å‡¦ç†ã—ãŸã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆ
        """
        # ASINã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        if self.args.output:
            output_path = Path(self.args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            with output_path.open('w', encoding='utf-8') as f:
                for asin in sorted(new_asins):
                    f.write(f"{asin}\n")

            self.log(f"  ASINãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {output_path}")
            self.log(f"  ä¿å­˜ä»¶æ•°: {len(new_asins)}ä»¶")

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if self.args.report:
            await self._generate_report(new_asins, categories)

    async def _generate_report(self, new_asins: Set[str], categories: List[tuple]):
        """
        ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Args:
            new_asins: æ–°è¦ASINã‚»ãƒƒãƒˆ
            categories: å‡¦ç†ã—ãŸã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆ
        """
        report_path = Path(self.args.report)
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with report_path.open('w', encoding='utf-8') as f:
            f.write(f"# ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ASINè‡ªå‹•æŠ½å‡ºãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write(f"**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write(f"## ğŸ“Š æŠ½å‡ºçµæœã‚µãƒãƒªãƒ¼\n\n")
            f.write(f"| æŒ‡æ¨™ | å€¤ |\n")
            f.write(f"|------|------|\n")
            f.write(f"| æ–°è¦ASINæ•° | {len(new_asins)}ä»¶ |\n")
            f.write(f"| ç·æŠ½å‡ºASINæ•° | {self.stats['total_extracted']}ä»¶ |\n")
            f.write(f"| é‡è¤‡ASINæ•° | {self.stats['duplicate_asins']}ä»¶ |\n")
            if self.stats['total_extracted'] > 0:
                f.write(f"| æ–°è¦ç‡ | {len(new_asins) / self.stats['total_extracted'] * 100:.1f}% |\n")
            f.write(f"| å‡¦ç†ã‚«ãƒ†ã‚´ãƒªæ•° | {self.stats['categories_processed']}ä»¶ |\n")
            f.write(f"\n")

            f.write(f"## ğŸ“‚ å‡¦ç†ã‚«ãƒ†ã‚´ãƒªä¸€è¦§\n\n")
            for i, (category, info) in enumerate(categories[:self.stats['categories_processed']]):
                f.write(f"{i+1}. **{category}**\n")
                f.write(f"   - ã‚µãƒ³ãƒ—ãƒ«å†…å•†å“æ•°: {info['count']}ä»¶\n")
                f.write(f"   - nodeIdPaths: `{info['nodeIdPaths']}`\n")
                f.write(f"\n")

            f.write(f"## âš™ï¸ å®Ÿè¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n\n")
            f.write(f"```\n")
            f.write(f"ç›®æ¨™æ–°è¦ASINæ•°: {self.args.target_new_asins}ä»¶\n")
            f.write(f"åˆæœŸã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {self.args.sample_size}ä»¶\n")
            f.write(f"ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šã®å–å¾—æ•°: {self.args.asins_per_category}ä»¶\n")
            f.write(f"æœ€å¤§ã‚«ãƒ†ã‚´ãƒªæ•°: {self.args.max_categories}ä»¶\n")
            f.write(f"è²©å£²æ•°ç¯„å›²: {self.args.sales_min} ä»¥ä¸Š\n")
            f.write(f"ä¾¡æ ¼ç¯„å›²: {self.args.price_min} ä»¥ä¸Š\n")
            f.write(f"å¸‚å ´: {self.args.market}\n")
            f.write(f"```\n")

        self.log(f"  ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="SellerSprite ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ASINè‡ªå‹•æŠ½å‡º",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ï¼ˆç›®æ¨™10,000ä»¶ã®æ–°è¦ASINï¼‰
  python sourcing/scripts/auto_extract_by_categories.py \\
    --target-new-asins 10000 \\
    --sample-size 1000 \\
    --asins-per-category 2000 \\
    --sales-min 300 \\
    --price-min 2500

  # ã‚«ãƒ†ã‚´ãƒªæ•°ã‚’åˆ¶é™ã—ã¦å®Ÿè¡Œ
  python sourcing/scripts/auto_extract_by_categories.py \\
    --target-new-asins 5000 \\
    --max-categories 10 \\
    --output category_asins.txt \\
    --report category_report.md
        """
    )

    # ç›®æ¨™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--target-new-asins",
        type=int,
        default=10000,
        help="ç›®æ¨™æ–°è¦ASINæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10000ï¼‰"
    )

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--sample-size",
        type=int,
        default=1000,
        help="åˆæœŸã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ã€æœ€å¤§: 2000ï¼‰"
    )
    parser.add_argument(
        "--asins-per-category",
        type=int,
        default=2000,
        help="å„ã‚«ãƒ†ã‚´ãƒªã®å–å¾—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2000ã€æœ€å¤§: 2000ï¼‰"
    )
    parser.add_argument(
        "--max-categories",
        type=int,
        default=20,
        help="æœ€å¤§ã‚«ãƒ†ã‚´ãƒªæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰"
    )

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--sales-min",
        type=int,
        default=300,
        help="æœˆé–“è²©å£²æ•°ã®æœ€å°å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300ï¼‰"
    )
    parser.add_argument(
        "--price-min",
        type=int,
        default=2500,
        help="ä¾¡æ ¼ã®æœ€å°å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2500ï¼‰"
    )
    parser.add_argument(
        "--market",
        type=str,
        default="JP",
        help="å¸‚å ´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: JPï¼‰"
    )

    # å‡ºåŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--output",
        type=str,
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆASINä¸€è¦§ã€æŒ‡å®šã—ãªã„å ´åˆã¯ä¿å­˜ã—ãªã„ï¼‰"
    )
    parser.add_argument(
        "--report",
        type=str,
        help="ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆMarkdownå½¢å¼ã€æŒ‡å®šã—ãªã„å ´åˆã¯ä¿å­˜ã—ãªã„ï¼‰"
    )

    args = parser.parse_args()

    # å®Ÿè¡Œ
    extractor = CategoryBasedExtractor(args)
    await extractor.run()


if __name__ == "__main__":
    asyncio.run(main())
