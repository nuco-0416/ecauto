"""
éå»ã®ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ„ãƒ¼ãƒ«

éå»ã®æŠ½å‡ºãƒ¬ãƒãƒ¼ãƒˆï¼ˆMarkdownï¼‰ã‹ã‚‰ã€ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®å–å¾—å±¥æ­´ã‚’
JSONå½¢å¼ã§ç”Ÿæˆã—ã¾ã™ã€‚

ä½¿ç”¨ä¾‹:
    python sourcing/scripts/generate_history_from_reports.py \
      --reports category_report_20251128.md category_report_additional_20251128.md category_report_round3_20251128.md \
      --output category_history.json \
      --pages-extracted 10
"""

import argparse
import json
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict


def parse_report(report_path: Path) -> List[Dict]:
    """
    ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’æŠ½å‡º

    Args:
        report_path: ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns:
        [{"category": "...", "nodeIdPaths": "...", "count": 10}, ...]
    """
    with open(report_path, 'r', encoding='utf-8') as f:
        content = f.read()

    categories = []

    # ã‚«ãƒ†ã‚´ãƒªä¸€è¦§ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
    category_section_match = re.search(
        r'## ğŸ“‚ å‡¦ç†ã‚«ãƒ†ã‚´ãƒªä¸€è¦§(.*?)(?=##|\Z)',
        content,
        re.DOTALL
    )

    if not category_section_match:
        print(f"[WARN] ã‚«ãƒ†ã‚´ãƒªä¸€è¦§ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {report_path}")
        return categories

    category_section = category_section_match.group(1)

    # å„ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º
    # ä¾‹: 1. **ãƒ‰ãƒ©ãƒƒã‚°ã‚¹ãƒˆã‚¢ > æ „é¤Šè£œåŠ©é£Ÿå“ > ...**
    category_pattern = re.compile(
        r'\d+\.\s+\*\*(.*?)\*\*.*?nodeIdPaths:\s+`(.*?)`',
        re.DOTALL
    )

    for match in category_pattern.finditer(category_section):
        category_name = match.group(1).strip()
        node_id_paths = match.group(2).strip()

        # ã‚µãƒ³ãƒ—ãƒ«å†…å•†å“æ•°ã‚‚æŠ½å‡ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        count_match = re.search(
            r'ã‚µãƒ³ãƒ—ãƒ«å†…å•†å“æ•°:\s+(\d+)ä»¶',
            match.group(0)
        )
        count = int(count_match.group(1)) if count_match else 0

        categories.append({
            'category': category_name,
            'nodeIdPaths': node_id_paths,
            'count': count
        })

    return categories


def generate_history(
    report_paths: List[Path],
    pages_extracted: int
) -> Dict:
    """
    è¤‡æ•°ã®ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ

    Args:
        report_paths: ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
        pages_extracted: å–å¾—æ¸ˆã¿ãƒšãƒ¼ã‚¸æ•°

    Returns:
        å±¥æ­´ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONå½¢å¼ï¼‰
    """
    history = {
        "categories": {},
        "metadata": {
            "total_asins": 0,
            "last_run": None,
            "generated_from_reports": [str(p) for p in report_paths],
            "generated_at": datetime.now().isoformat()
        }
    }

    # å„ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡¦ç†
    for report_path in report_paths:
        print(f"å‡¦ç†ä¸­: {report_path}")
        categories = parse_report(report_path)
        print(f"  â†’ {len(categories)}ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º")

        for cat_info in categories:
            category_name = cat_info['category']

            # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ€åˆã®ãƒ¬ãƒãƒ¼ãƒˆã‚’å„ªå…ˆï¼‰
            if category_name in history['categories']:
                continue

            history['categories'][category_name] = {
                'nodeIdPaths': cat_info['nodeIdPaths'],
                'pages_extracted': pages_extracted,
                'last_updated': datetime.now().isoformat(),
                'asins_count': 0,  # å®Ÿéš›ã®ASINæ•°ã¯ä¸æ˜ãªã®ã§0
                'sample_count': cat_info['count']
            }

    print(f"\nåˆè¨ˆ {len(history['categories'])} ã‚«ãƒ†ã‚´ãƒªã‚’å±¥æ­´ã«è¿½åŠ ")

    return history


def main():
    parser = argparse.ArgumentParser(
        description="éå»ã®ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        "--reports",
        nargs='+',
        required=True,
        help="ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆè¤‡æ•°æŒ‡å®šå¯èƒ½ï¼‰"
    )

    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="å‡ºåŠ›å…ˆã®å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆJSONå½¢å¼ï¼‰"
    )

    parser.add_argument(
        "--pages-extracted",
        type=int,
        default=10,
        help="å–å¾—æ¸ˆã¿ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰"
    )

    args = parser.parse_args()

    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å¤‰æ›
    report_paths = [Path(r) for r in args.reports]

    # å­˜åœ¨ç¢ºèª
    for path in report_paths:
        if not path.exists():
            print(f"[ERROR] ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
            return

    # å±¥æ­´ã‚’ç”Ÿæˆ
    history = generate_history(report_paths, args.pages_extracted)

    # JSONå½¢å¼ã§ä¿å­˜
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_path}")
    print(f"   ã‚«ãƒ†ã‚´ãƒªæ•°: {len(history['categories'])}ä»¶")
    print(f"   å–å¾—æ¸ˆã¿ãƒšãƒ¼ã‚¸: {args.pages_extracted}ãƒšãƒ¼ã‚¸/ã‚«ãƒ†ã‚´ãƒª")


if __name__ == "__main__":
    main()
